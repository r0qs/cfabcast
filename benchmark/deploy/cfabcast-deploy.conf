include "application"

akka {
  # To turn off logging
  #stdout-loglevel = "OFF"
  #loglevel = "OFF"
  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  loglevel = "WARNING"
  log-dead-letters = off
  log-dead-letters-during-shutdown = off
  log-config-on-start = off

  actor {
    cfabcast-dispatcher {
      # Dispatcher is the name of the event-based dispatcher
      type = Dispatcher

      # What kind of ExecutionService to use
      executor = "thread-pool-executor"

      # Configuration for the thread pool
      thread-pool-executor {
        # minimum number of threads to cap factor-based core number to
        core-pool-size-min = 2
        # No of core threads ... ceil(available processors * factor)
        core-pool-size-factor = 2.0
        # maximum number of threads to cap factor-based number to
        core-pool-size-max = 10
      }
      # Throughput defines the maximum number of messages to be
      # processed per actor before the thread jumps to the next actor.
      # Set to 1 for as fair as possible.
      throughput = 100
    }
  
    cfabcast-default-dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-factor = 2.0
        parallelism-max = 5
      }
      throughput = 1
    }
  }

  remote {
    log-sent-messages = off
    log-received-messages = off
    log-remote-lifecycle-events = on
  }

  cluster {
    roles = [cfabcast]

    # Set seed node in a enviromment variable
    seed-nodes = []

    auto-down-unreachable-after = off

    # Settings for the Phi accrual failure detector (http://ddg.jaist.ac.jp/pub/HDY+04.pdf
    # [Hayashibara et al]) used by the cluster subsystem to detect unreachable
    # members.
    # The default PhiAccrualFailureDetector will trigger if there are no heartbeats within
    # the duration heartbeat-interval + acceptable-heartbeat-pause + threshold_adjustment,
    # i.e. around 5.5 seconds with default settings.
    failure-detector {
 
      # FQCN of the failure detector implementation.
      # It must implement akka.remote.FailureDetector and have
      # a public constructor with a com.typesafe.config.Config and
      # akka.actor.EventStream parameter.
      implementation-class = "akka.remote.PhiAccrualFailureDetector"
 
      # How often keep-alive heartbeat messages should be sent to each connection.
      # default: 1 s
      heartbeat-interval = 5 s
 
      # Defines the failure detector threshold.
      # A low threshold is prone to generate many wrong suspicions but ensures
      # a quick detection in the event of a real crash. Conversely, a high
      # threshold generates fewer mistakes but needs more time to detect
      # actual crashes.
      # default : 8.0
      threshold = 12.0
 
      # Number of the samples of inter-heartbeat arrival times to adaptively
      # calculate the failure timeout for connections.
      # default: 1000
      max-sample-size = 1000
 
      # Minimum standard deviation to use for the normal distribution in
      # AccrualFailureDetector. Too low standard deviation might result in
      # too much sensitivity for sudden, but normal, deviations in heartbeat
      # inter arrival times.
      # default: 100 ms
      min-std-deviation = 100 ms
 
      # Number of potentially lost/delayed heartbeats that will be
      # accepted before considering it to be an anomaly.
      # This margin is important to be able to survive sudden, occasional,
      # pauses in heartbeat arrivals, due to for example garbage collect or
      # network drop.
      # default: 3 s 
      acceptable-heartbeat-pause = 60 s
 
      # Number of member nodes that each member will send heartbeat messages to,
      # i.e. each node will be monitored by this number of other nodes.
      # default: 5
      monitored-by-nr-of-members = 1
      
      # After the heartbeat request has been sent the first failure detection
      # will start after this period, even though no heartbeat message has
      # been received.
      # default: 1 s
      expected-response-after = 20 s
    }
  }

#  persistence {
#     journal.plugin = "akka.persistence.journal.leveldb-shared"
#     journal.leveldb-shared.store {
        # FIXME: DO NOT USE 'native = off' IN PRODUCTION !
#        native = off
#        dir = "target/shared-journal"
#     }
#     snapshot-store.local.dir = "target/snapshots"
#  }
}

#akka.actor.deployment {
#  /user/node {
#    dispatcher = cfabcast-dispatcher
#  }

#  "/user/node/proposer*" {
#    dispatcher = cfabcast-dispatcher
#  }

#  "/user/node/acceptor*" {
#    dispatcher = cfabcast-dispatcher
#  }

#  "/user/node/learner*" {
#    dispatcher = cfabcast-dispatcher
#  }
#}

#kamon.statsd.hostname = "200.131.199.17"
#kamon.statsd.port = 8125

cfabcast {
  # Minimum required number of replicas before start computation
  min-nr-of-nodes = 10
  
  node-id = ""

  # Determines the size of the Quorum of Acceptors
  # (nr-of-acceptors / 2) + 1
  quorum-size = 6

  # Select one of different kinds of deliveries. The options are:
  # "conservative" : only deliver the value map when vmap is complete(all values have a quorum)
  # "optimistic": deliver each single map when possible(each value have a quorum)
  # "super-optimistic": deliver the single map when become aware of it (no quorum)
  delivery-policy = "optimistic"

  role {
    # Minimum required number of agents of a certain role before start the protocol
    # E.g. to require 2 agents with role 'cfproposer':
    #   cfproposer.min-nr-of-agents = 2
    #
    #<role-name>.min-nr-of-agents = 1
    
    # 1 == classic paxos
    cfproposer.min-nr-of-agents = 1 
  }

  nodes {
    # Per node configuration  
    node42 {
      hostname = "node42"
      port = 2551
      roles = [proposer, acceptor, learner]
      # Number of agents that play a certain role in this node.
      # E.g. to specify a node that play the role of 1 proposer and 1 learner:
      #  proposer.nr-of-agents  = 1
      #  acceptor.nr-of-agents  = 0
      #  learner.nr-of-agents   = 1
      proposer {
        nr-of-agents  = 1
        proposer1.id = "p1"
      }

      learner {
        nr-of-agents   = 1
        learner1.id = "l1"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor1.id = "a1"
      }
    }
  
    node43 {
      hostname = "node43"
      port = 2552
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer2.id = "p2"
      }

      learner {
        nr-of-agents   = 1
        learner2.id = "l2"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor2.id = "a2"
      }
    }
  
    node44 {
      hostname = "node44"
      port = 2553
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer3.id = "p3"
      }

      learner {
        nr-of-agents   = 1
        learner3.id = "l3"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor3.id = "a3"
      }
    }
    node45 {
      hostname = "node45"
      port = 2554
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer4.id = "p4"
      }

      learner {
        nr-of-agents   = 1
        learner4.id = "l4"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor4.id = "a4"
      }
    }
    node46 {
      hostname = "node46"
      port = 2555
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer5.id = "p5"
      }

      learner {
        nr-of-agents   = 1
        learner5.id = "l5"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor5.id = "a5"
      }
    }

    node47 {
      hostname = "node47"
      port = 2556
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer6.id = "p6"
      }

      learner {
        nr-of-agents   = 1
        learner6.id = "l6"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor6.id = "a6"
      }
    }

    node48 {
      hostname = "node48"
      port = 2557
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer7.id = "p7"
      }

      learner {
        nr-of-agents   = 1
        learner7.id = "l7"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor7.id = "a7"
      }
    }

    node49 {
      hostname = "node49"
      port = 2558
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer8.id = "p8"
      }

      learner {
        nr-of-agents   = 1
        learner8.id = "l8"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor8.id = "a8"
      }
    }

    node50 {
      hostname = "node50"
      port = 2559
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer9.id = "p9"
      }

      learner {
        nr-of-agents   = 1
        learner9.id = "l9"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor9.id = "a9"
      }
    }

    node51 {
      hostname = "node51"
      port = 2560
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer10.id = "p10"
      }

      learner {
        nr-of-agents   = 1
        learner10.id = "l10"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor10.id = "a10"
      }
    }
    node52 {
      hostname = "node52"
      port = 2561
      roles = [proposer, acceptor, learner]
      proposer {
        nr-of-agents  = 1
        proposer11.id = "p11"
      }

      learner {
        nr-of-agents   = 1
        learner11.id = "l11"
      }

      acceptor {
        nr-of-agents  = 1
        acceptor11.id = "a11"
      }
    }
  }
}
